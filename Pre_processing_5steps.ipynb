{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jiaye39/TimeSeriesAnalysis/blob/main/Pre_processing_5steps.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEXhmR2NDad1"
      },
      "source": [
        "# Pre_processing_5steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2E_7HFE6GtNF"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "r = requests.get(\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\")\n",
        "imdb_tgz = r.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRhc7AonG0yT"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import re\n",
        "import tarfile\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "good_file = re.compile(r\"^aclImdb/(test|train)/(pos|neg)/.*\\.txt$\")\n",
        "\n",
        "with tarfile.open(fileobj=io.BytesIO(r.content), mode=\"r:gz\") as tgz:\n",
        "    all_members = tgz.getmembers()\n",
        "    data_files = list(filter(lambda x: x.isfile() and good_file.match(x.name) is not None, all_members))\n",
        "    for f in tqdm(data_files):\n",
        "        tgz.extract(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYee4ksMJwUJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_files\n",
        "train_data, test_data = load_files(\"./aclImdb/train\", encoding=\"utf-8\"), load_files(\"./aclImdb/test\", encoding=\"utf-8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHCGzA0yZJsr"
      },
      "outputs": [],
      "source": [
        "label2txt = {label: txt for label, txt in enumerate(train_data.target_names)}\n",
        "txt2label = {txt: label for label, txt in label2txt.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2GFOhVcI5qX"
      },
      "outputs": [],
      "source": [
        "type(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "on9UqcR3I5nb"
      },
      "outputs": [],
      "source": [
        "X_train, y_train = train_data.data, train_data.target\n",
        "X_test, y_test = test_data.data, test_data.target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni6uEf_mbX-7"
      },
      "source": [
        "# The data\n",
        "* `X_train` and `X_test` are `list` of 25000 `str` texts\n",
        "* `y_train` and `y_test` are `list` of 25000 `int`, either `0` (negative review) or `1` (positive review)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VY6yBb0GXdfO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "print(\"TRAIN data:\")\n",
        "print(\"class balance: \", np.bincount(y_train))\n",
        "print()\n",
        "print(\"TEST data:\")\n",
        "print(\"class balance: \", np.bincount(y_train))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkBkH7n2lrfi"
      },
      "outputs": [],
      "source": [
        "!pip install eli5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yY52ehlPC5Xe"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6gP_pl3DaQu"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEzsPDfPed9f"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "!pip install gensim\n",
        "from gensim.corpora import Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import rich\n",
        "\n",
        "from rich.console import Console\n",
        "from rich.table import Table"
      ],
      "metadata": {
        "id": "ii71vzhkibtY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "cBAE2ixC2cnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1hoSESKgD0f"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVrh9caUgCvm"
      },
      "outputs": [],
      "source": [
        "def create_table(dictionary: Dictionary, n: int = 10) -> Table:\n",
        "    table = Table(title=f\"Top-{n} Most Frequent Tokens ({d.num_docs} documents, {d.num_pos} tokens, {len(d)} words in dictionary)\")\n",
        "\n",
        "    table.add_column(\"Token\", justify=\"left\", no_wrap=True)\n",
        "    table.add_column(\"Corpus Frequency\", justify=\"right\")\n",
        "    table.add_column(\"% of Tokens\", justify=\"right\")\n",
        "    table.add_column(\"Document Frequency\", justify=\"right\")\n",
        "    table.add_column(\"% of Documents\", justify=\"right\")\n",
        "\n",
        "    for token, frequency in d.most_common(n=n):\n",
        "        percent_tokens = frequency / d.num_pos\n",
        "        doc_frequency = d.dfs[d.token2id[token]]\n",
        "        percent_doc = doc_frequency / d.num_docs\n",
        "        table.add_row(token, str(frequency), f\"{percent_tokens:.1%}\", str(doc_frequency), f\"{percent_doc:.1%}\")\n",
        "\n",
        "    return table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liQjE4xRXXKf"
      },
      "source": [
        "## 1st Pre-Processing: Lowercase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZATYTGL-XfJo"
      },
      "outputs": [],
      "source": [
        "def lowercase(text: str) -> list[str]:\n",
        "    \"\"\"Tokenize and preprocess the text.\n",
        "\n",
        "    Normalize the text to lowercase\n",
        "\n",
        "    Returns:\n",
        "        tokens: a list of tokens\n",
        "    \"\"\"\n",
        "    return [x.lower() for x in word_tokenize(text)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "hz3zbuLmXQyk"
      },
      "outputs": [],
      "source": [
        "X_train_tokenized = [lowercase(x) for x in tqdm(X_train)]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d = Dictionary(X_train_tokenized)"
      ],
      "metadata": {
        "id": "-uuwcGIiq1l_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "kkTBoIE8X-YC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "outputId": "5a40b372-6830-4e7d-d6cb-a952f01792a0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[3m Top-20 Most Frequent Tokens (25000 documents, 7065344 tokens, 111720 words in  \u001b[0m\n",
              "\u001b[3m                                  dictionary)                                   \u001b[0m\n",
              "┏━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mToken\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mCorpus Frequency\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m% of Tokens\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mDocument Frequency\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m% of Documents\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩\n",
              "│ the   │           334840 │        4.7% │              24791 │          99.2% │\n",
              "│ ,     │           275887 │        3.9% │              24054 │          96.2% │\n",
              "│ .     │           236573 │        3.3% │              24685 │          98.7% │\n",
              "│ and   │           163477 │        2.3% │              24159 │          96.6% │\n",
              "│ a     │           162304 │        2.3% │              24173 │          96.7% │\n",
              "│ of    │           145404 │        2.1% │              23721 │          94.9% │\n",
              "│ to    │           135153 │        1.9% │              23459 │          93.8% │\n",
              "│ is    │           110284 │        1.6% │              22522 │          90.1% │\n",
              "│ /     │           102115 │        1.4% │              14711 │          58.8% │\n",
              "│ >     │           102036 │        1.4% │              14681 │          58.7% │\n",
              "│ <     │           101971 │        1.4% │              14671 │          58.7% │\n",
              "│ br    │           101871 │        1.4% │              14666 │          58.7% │\n",
              "│ it    │            95024 │        1.3% │              22207 │          88.8% │\n",
              "│ in    │            93197 │        1.3% │              22004 │          88.0% │\n",
              "│ i     │            86747 │        1.2% │              19825 │          79.3% │\n",
              "│ this  │            75566 │        1.1% │              22607 │          90.4% │\n",
              "│ that  │            73011 │        1.0% │              20292 │          81.2% │\n",
              "│ 's    │            62167 │        0.9% │              17895 │          71.6% │\n",
              "│ was   │            50383 │        0.7% │              16377 │          65.5% │\n",
              "│ as    │            46834 │        0.7% │              16103 │          64.4% │\n",
              "└───────┴──────────────────┴─────────────┴────────────────────┴────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\"> Top-20 Most Frequent Tokens (25000 documents, 7065344 tokens, 111720 words in  </span>\n",
              "<span style=\"font-style: italic\">                                  dictionary)                                   </span>\n",
              "┏━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Token </span>┃<span style=\"font-weight: bold\"> Corpus Frequency </span>┃<span style=\"font-weight: bold\"> % of Tokens </span>┃<span style=\"font-weight: bold\"> Document Frequency </span>┃<span style=\"font-weight: bold\"> % of Documents </span>┃\n",
              "┡━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩\n",
              "│ the   │           334840 │        4.7% │              24791 │          99.2% │\n",
              "│ ,     │           275887 │        3.9% │              24054 │          96.2% │\n",
              "│ .     │           236573 │        3.3% │              24685 │          98.7% │\n",
              "│ and   │           163477 │        2.3% │              24159 │          96.6% │\n",
              "│ a     │           162304 │        2.3% │              24173 │          96.7% │\n",
              "│ of    │           145404 │        2.1% │              23721 │          94.9% │\n",
              "│ to    │           135153 │        1.9% │              23459 │          93.8% │\n",
              "│ is    │           110284 │        1.6% │              22522 │          90.1% │\n",
              "│ /     │           102115 │        1.4% │              14711 │          58.8% │\n",
              "│ &gt;     │           102036 │        1.4% │              14681 │          58.7% │\n",
              "│ &lt;     │           101971 │        1.4% │              14671 │          58.7% │\n",
              "│ br    │           101871 │        1.4% │              14666 │          58.7% │\n",
              "│ it    │            95024 │        1.3% │              22207 │          88.8% │\n",
              "│ in    │            93197 │        1.3% │              22004 │          88.0% │\n",
              "│ i     │            86747 │        1.2% │              19825 │          79.3% │\n",
              "│ this  │            75566 │        1.1% │              22607 │          90.4% │\n",
              "│ that  │            73011 │        1.0% │              20292 │          81.2% │\n",
              "│ 's    │            62167 │        0.9% │              17895 │          71.6% │\n",
              "│ was   │            50383 │        0.7% │              16377 │          65.5% │\n",
              "│ as    │            46834 │        0.7% │              16103 │          64.4% │\n",
              "└───────┴──────────────────┴─────────────┴────────────────────┴────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "console = Console(record=True, width=80)\n",
        "t = create_table(d, 20)\n",
        "console.print(t, justify=\"center\")\n",
        "console.save_svg(\"all_vocab.svg\", title=\"Vocabulary\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6F7U8kXYZJJ"
      },
      "source": [
        "## 2nd Preprocessing: lowercase + stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "JVSxZgF4Y2WN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6af62714-b08d-4fb8-a5eb-5649655531eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "nltk.download(\"stopwords\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkrV-S4FYpp0"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopwords = set(stopwords.words(\"english\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CL37Qaa7Y5tP"
      },
      "outputs": [],
      "source": [
        "print(stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQaKtK_0YQXR"
      },
      "outputs": [],
      "source": [
        "def lower_stop(text: str, stopwords: set[str]) -> list[str]:\n",
        "    \"\"\"Tokenize and preprocess the text.\n",
        "\n",
        "    Normalize the text to lowercase and remove stopwords.\n",
        "\n",
        "    Returns:\n",
        "        tokens: a list of tokens\n",
        "    \"\"\"\n",
        "    tokens = lowercase(text)\n",
        "    return [x for x in tokens if x not in stopwords]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XaakvWqZXAB"
      },
      "outputs": [],
      "source": [
        "X_train_tokenized = [lower_stop(text=x, stopwords=stopwords) for x in tqdm(X_train)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSjG12_AYzNb"
      },
      "outputs": [],
      "source": [
        "console = Console(record=True, width=120)\n",
        "d = Dictionary(X_train_tokenized)\n",
        "t = create_table(d, 20)\n",
        "console.print(t)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztAPEGHEcHDn"
      },
      "source": [
        "## 3rd Preprocessing: accept only alphabetical tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBLfLZuocEny"
      },
      "outputs": [],
      "source": [
        "def lower_stop_only_alpha(text: str, stopwords: set[str]) -> list[str]:\n",
        "    \"\"\"Tokenize and preprocess the text.\n",
        "\n",
        "    Normalize the text to lowercase, remove stopwords, remove tokens\n",
        "    that are not entirely made of letters, remove tokens with only 1 character.\n",
        "\n",
        "    Returns:\n",
        "        tokens: a list of tokens\n",
        "    \"\"\"\n",
        "    own_stopwords = {\"br\"}\n",
        "    tokens = lower_stop(text=text, stopwords=stopwords | own_stopwords)\n",
        "    return [x for x in tokens if x.isalpha() and len(x) > 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLYutMAYcEk2"
      },
      "outputs": [],
      "source": [
        "X_train_tokenized = [lower_stop_only_alpha(text=x, stopwords=stopwords) for x in tqdm(X_train)]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d = Dictionary(X_train_tokenized)"
      ],
      "metadata": {
        "id": "0teXk2NHsuAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cO7v1VqgcEiM"
      },
      "outputs": [],
      "source": [
        "console = Console(record=True, width=120)\n",
        "t = create_table(d, 20)\n",
        "console.print(t)\n",
        "console.save_svg(\"alpha_vocab.svg\", title=\"Vocabulary\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks50EedJDrVy"
      },
      "source": [
        "## 4th Preprocessing - Stemming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uM-fP0aDv2S"
      },
      "source": [
        "* Reduces the vocabulary by reducing plurals, conjugations to their root form\n",
        "* \"making\", \"makes\", \"made\" are transformed into \"make\"\n",
        "* \"cats\", \"cat\" transform into \"cat\"\n",
        "* **but** \"movie\" is transformed into \"movi\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRJVoZvZZH2r"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvigSf-kEPk1"
      },
      "outputs": [],
      "source": [
        "porter = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKIVKY6CESX5"
      },
      "outputs": [],
      "source": [
        "porter.stem(\"cats\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4brGdor6EUoJ"
      },
      "outputs": [],
      "source": [
        "porter.stem(\"making\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCtnmIn1GUDr"
      },
      "outputs": [],
      "source": [
        "porter.stem(\"movie\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnf85ZeuEgxw"
      },
      "outputs": [],
      "source": [
        "def lower_stop_only_alpha_stem(text: str, stopwords: list[str]) -> list[str]:\n",
        "    tokens = lower_stop_only_alpha(text, stopwords)\n",
        "    return [porter.stem(x) for x in tokens]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFO9KW0CFhg8"
      },
      "outputs": [],
      "source": [
        "X_train_tokenized = [lower_stop_only_alpha_stem(text=x, stopwords=stopwords) for x in tqdm(X_train)]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d = Dictionary(X_train_tokenized)\n"
      ],
      "metadata": {
        "id": "JmMjsblb6fGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9Z8XotIFhg-"
      },
      "outputs": [],
      "source": [
        "console = Console(record=True, width=120)\n",
        "t = create_table(d, 20)\n",
        "console.print(t)\n",
        "console.save_svg(\"stem.svg\", title=\"Vocabulary\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIXZJaLhIpNi"
      },
      "source": [
        "## 5th Preprocessing - NGrams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjJyAZ2ZIsBQ"
      },
      "source": [
        "* 2-grams are made of 2 consecutive tokens in the text\n",
        "* \"the cat is blue\" has 2 grams `[\"the\", \"cat\"], [\"cat\", \"is\"], [\"is\", \"blue\"]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lkU1PbzJ8Ty"
      },
      "outputs": [],
      "source": [
        "from more_itertools import windowed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOccEWIDJnKO"
      },
      "outputs": [],
      "source": [
        "def get_ngrams(tokens: list[str], n: int) -> list[str]:\n",
        "    return [\" \".join(x) for x in windowed(tokens, n=n)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLNZgZp6JnG3"
      },
      "outputs": [],
      "source": [
        "tokens = lower_stop(\"I live in new york city\", [])\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugZSB2WBJnEa"
      },
      "outputs": [],
      "source": [
        "print(get_ngrams(tokens, n=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNVvVuC9JnB3"
      },
      "outputs": [],
      "source": [
        "print(get_ngrams(tokens, n=3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZ2JxukUJm_T"
      },
      "outputs": [],
      "source": [
        "def lower_stop_only_alpha_stem_ngrams(text: str, stopwords: list[str], ngrams: int) -> list[str]:\n",
        "    tokens = lower_stop_only_alpha_stem(text, stopwords)\n",
        "    n_grams = get_ngrams(tokens, ngrams)\n",
        "    return n_grams\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRzqQxxTJm8q"
      },
      "outputs": [],
      "source": [
        "lower_stop_only_alpha_stem_ngrams(\"I live in the middle of new york city.\", stopwords=stopwords, ngrams=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUB7G9ccJm6R"
      },
      "outputs": [],
      "source": [
        "X_train_tokenized = [lower_stop_only_alpha_stem_ngrams(x, stopwords, 2) for x in tqdm(X_train)]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d = Dictionary(X_train_tokenized)"
      ],
      "metadata": {
        "id": "COy7dJti89ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRmVLCD_Lv4m"
      },
      "outputs": [],
      "source": [
        "console = Console(record=True, width=120)\n",
        "t = create_table(d, 20)\n",
        "console.print(t)\n",
        "console.save_svg(\"bigrams.svg\", title=\"Vocabulary\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MIg22fhLv1g"
      },
      "outputs": [],
      "source": [
        "t = create_table(d, 200)\n",
        "console.print(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Byh_UYY0Lvy8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BK_4p2mkLvwh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXq3ujDCLvt_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFBQOWJnLvro"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHeG8nvwIFa_"
      },
      "source": [
        "# Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rULDmgt3IIQA"
      },
      "source": [
        "* Turn the 2-step process into a Pipeline\n",
        "* Hyperparameters:\n",
        "  * `C` of logistic regression\n",
        "  * `analyzer` of vectorizer, to select the text pre-processing\n",
        "  * `ngram_range` of vectorizer, to select ngrams\n",
        "* Use GridSearchCV to identify the best hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaiiFFV00w5Z"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYgpyOjUFoWl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVE-FeCV0Vt6"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "porter = PorterStemmer()\n",
        "\n",
        "def stop(text: str) -> list[str]:\n",
        "    tokens = word_tokenize(text)\n",
        "    return [x for x in tokens if x not in stop_words]\n",
        "\n",
        "def stop_stem(text: str) -> list[str]:\n",
        "    tokens = stop(text)\n",
        "    return [porter.stem(x) for x in tokens]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTGu1nEozYGv"
      },
      "outputs": [],
      "source": [
        "pipe = make_pipeline(\n",
        "    CountVectorizer(lowercase=True), LogisticRegression(max_iter=10000)\n",
        ")\n",
        "\n",
        "param_grid = {\n",
        "    \"logisticregression__C\": np.logspace(-2, 2, 5),\n",
        "    \"countvectorizer__ngram_range\": [(1, 1), (1, 2)],\n",
        "    \"countvectorizer__analyzer\": [stop, stop_stem]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(pipe, param_grid=param_grid, verbose=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ap2o3XBG2En_"
      },
      "outputs": [],
      "source": [
        "grid.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8WuRKz12HEz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}