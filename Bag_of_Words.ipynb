{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNj+shORGwlKGYKH+Nz/UWc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jiaye39/TimeSeriesAnalysis/blob/main/Bag_of_Words.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bag of Words\n",
        "We study 2 types of BoW vectors:\n",
        "* **Raw Count**: actually count the number of occurences of each word in a text\n",
        "* **TF-IDF**: adjust the raw count to favor words that appear a lot in a few documents, as opposed to those who appear a lot in all documents\n",
        "* 关联package: NLTL sapcy tqdm eli5 gensim rich\n",
        "* 介绍库基本用法+原理，实践回归+分析回归权重 (词性的neg或pos)"
      ],
      "metadata": {
        "id": "2rGDMt-GD26_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**github无法显示output---进colab重新运行**"
      ],
      "metadata": {
        "id": "g3EhYQC7cxp6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basic import"
      ],
      "metadata": {
        "id": "XUC_VyfeziZZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_6kius3yORa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import requests"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Corpus"
      ],
      "metadata": {
        "id": "0cZrFf_-zbDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "r=requests.get('https://sherlock-holm.es/stories/plain-text/scan.txt')\n",
        "\n",
        "assert r.status_code == 200\n",
        "\n",
        "with open('scandal_in_bohemia.txt', 'w') as out:\n",
        "    out.write(r.content.decode('utf-8'))\n",
        "lines = [txt for txt in open('scandal_in_bohemia.txt') if len(txt.strip()) > 0]\n",
        "\n",
        "print(lines[:20])"
      ],
      "metadata": {
        "id": "kO72L4gxzap3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define First Paragraph\n",
        "par=''.join([x.strip() for x in lines[7:25]])"
      ],
      "metadata": {
        "id": "LiDjdhz3z-Li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLTK\n",
        "*   punkt：这是一个预训练的分词器模型，用于将文本分割成句子和单词。这是许多NLP任务的基础步\n",
        "*  punkt_tab：这是另一个与punkt相关的分词器，可能用于处理特定格式的文本或提供额外的分词功能\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Z17Hb6D20Ttq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "d9Vrvbjj0XRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   sent_tokenize: The sentence tokenizer takes care to split a text into sentences.\n",
        "*   word_tokenize: The word tokenizer takes care to split a text into words.\n",
        "\n",
        "*   拆解文本->向量化\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mU5MsslN02WV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk_sentences=sent_tokenize(par)\n",
        "nltk_words=word_tokenize(par)\n",
        "print(nltk_sentences,'\\n')\n",
        "print(nltk_words)"
      ],
      "metadata": {
        "id": "DON7Nik40_tR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spacy\n",
        "\n",
        "\n",
        "*   en_core_web_sm 包含处理en文本的所有组件与数据\n",
        "*   zh_core_web_sm 处理中文zh版本\n",
        "\n"
      ],
      "metadata": {
        "id": "PzkHgT8v1g83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp=spacy.load('en_core_web_sm')\n",
        "doc=nlp(par)"
      ],
      "metadata": {
        "id": "JOVPAM0z1kU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   spacy_sentences (doc.sents) : 文本拆解为句子 -> 向量化\n",
        "*   spacy_tokens (x for x in xxxx[i]) : 句子拆解为tokens -> 向量化\n",
        "\n"
      ],
      "metadata": {
        "id": "WvRPFIgL3HDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_sentences=list(doc.sents)\n",
        "spacy_tokens=[x for x in spacy_sentences[0]]\n",
        "print(spacy_sentences,'\\n')\n",
        "print(spacy_tokens)"
      ],
      "metadata": {
        "id": "B9cLIpT63Gvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sklearn Generalities （CountVectorizer&TfidfVectorizer）\n",
        "Classes likes `CountVectorizer` or `TfidfVectorizer` works in the following way:\n",
        "* Instantiate an object with specific parameters (`v = CountVectorizer(...)`)\n",
        "* Fit this object to your corpus = learn the vocabulary (method `v.fit(...)`)\n",
        "* Transform any piece of text you have into a vector (method `v.transform()`)\n",
        "*   **用CountVectorizer或者TfidfVectorizer做特征提取，文本转化为bow后才可以进行回归等操作**"
      ],
      "metadata": {
        "id": "SwO24rpG-Emv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer"
      ],
      "metadata": {
        "id": "UQ0R67V9_FDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "def 2 function for below analysis"
      ],
      "metadata": {
        "id": "8GqQOxch_Ig0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_vocabulary(vectorizer):\n",
        "    words = vectorizer.get_feature_names_out()\n",
        "\n",
        "    print(f'Vocabulary size: {len(words)} words')\n",
        "\n",
        "    # we can print ~10 words per line\n",
        "    for l in np.array_split(words, math.ceil(len(words) / 10)):\n",
        "        print(''.join([f'{x:<15}' for x in l]))"
      ],
      "metadata": {
        "id": "h3uFIPBL-FDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxgmPDksgFP-"
      },
      "source": [
        "import os\n",
        "os.environ[\"FORCE_COLOR\"] = \"1\"\n",
        "\n",
        "from termcolor import colored\n",
        "\n",
        "def show_bow(vectorizer, bow):\n",
        "    words = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # we can print ~8 words + coefs per line\n",
        "    for l in np.array_split(list(zip(words, bow)), math.ceil(len(words) / 8)):\n",
        "        print(' | '.join([colored(f'{w:<15}:{n:>2}', 'grey') if int(n) == 0 else colored(f'{w:<15}:{n:>2}', on_color='on_yellow', attrs=['bold']) for w, n in l ]))\n",
        "\n",
        "def show_bow_float(vectorizer, bow):\n",
        "    words = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # we can print ~6 words + coefs per line\n",
        "    for l in np.array_split(list(zip(words, bow)), math.ceil(len(words) / 6)):\n",
        "        print(' | '.join([colored(f'{w:<15}:{float(n):>0.2f}', 'grey') if float(n) == 0 else colored(f'{w:<15}:{float(n):>0.2f}', on_color='on_yellow', attrs=['bold']) for w, n in l ]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Raw Count\n",
        "* We take a text, any text, and represent it as a vector\n",
        "* Each text is represented by a vector with **N** dimensions\n",
        "* Each dimension is representative of **1 word** of the vocabulary\n",
        "* The coefficient in dimension **k** is the number of times the word at index **k** in the vocabulary is seen in the represented text"
      ],
      "metadata": {
        "id": "x4SlAhQe-zI4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eg: Reduced Vocabulary. (corpus: 1st paragraph of book, document:1 sentence)"
      ],
      "metadata": {
        "id": "_dJLJ7pZ_QFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_small= CountVectorizer(lowercase=True)\n",
        "count_small.fit(nltk_sentences)\n",
        "show_vocabulary(count_small)"
      ],
      "metadata": {
        "id": "uSP6hgpq-67D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The option `lowercase` sets up one behavior of the raw count: do we consider `And` to be different than `and`?\n",
        "\n",
        "* `lowercase=False` gives 134 unique words in the vocabulary\n",
        "* `lowercase=True` gives 127 unique words"
      ],
      "metadata": {
        "id": "YXUpqJDQ_2rU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s = nltk_sentences[0]\n",
        "\n",
        "print(f'Text: \"{s}\"')\n",
        "bow = count_small.transform([s])\n",
        "print(f'BoW Shape: {bow.shape}')\n",
        "bow = bow.toarray()   # From sparse matrix to dense matrix (Careful with MEMORY)\n",
        "print(f'BoW Vector: {bow}')"
      ],
      "metadata": {
        "id": "GJT5BKEbAQi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_bow(count_small,bow[0])"
      ],
      "metadata": {
        "id": "xOgjhpPSAEeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF\n",
        "The basic for TF-IDF is that cosine similarity with raw count coefficients puts too much emphasis on the number of occurences of a word within a document.\n",
        "\n",
        "Repeating a word will artifically increase the cosine similarity with any text containing this word.\n",
        "\n",
        "Consider which word would be important:\n",
        "1. One that is repeated a lot and equally present in each document\n",
        "1. One that appears a lot only in a few document\n",
        "TF-IDF computes coefficients:\n",
        "* Low values for common words (ie present in the document, but quite common over the corpus)\n",
        "* High values for uncommon words (ie present in the document, but not common over the corpus)\n",
        "\n",
        "We consider one specific document, and one specific word.\n",
        "\n",
        "* **TF = Term Frequency**: the number of times the word appears in the document\n",
        "* **DF = Document Frequency**: the number of document in the corpus, in which the word appears\n",
        "* **IDF = Inverse Document Frequency**: the inverse of the Document Frequency.\n",
        "\n",
        "Logarithms are introduced, to reflect that 100 times a word does not deliver 100 times the information.\n",
        "\n",
        "Given a word **w**, a document **d** in a corpus of **D** documents:\n",
        "\n",
        "$\\textrm{TF-IDF(w, d) = TF(w, d) * IDF(w)}$\n",
        "\n",
        "$\n",
        "\\begin{align}\n",
        "\\textrm{IDF(w) = log} \\left( \\frac{1 + \\textrm{D}}{1 + \\textrm{DF(w)}} \\right) + 1\n",
        "\\end{align}\n",
        "$"
      ],
      "metadata": {
        "id": "oYwXys07AaSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf=TfidfVectorizer()\n",
        "tfidf.fit(nltk_sentences)\n",
        "show_vocabulary(tfidf)"
      ],
      "metadata": {
        "id": "Q0R2-1O5A9Ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = nltk_sentences[0]\n",
        "\n",
        "print(f'Text: \"{s}\"')\n",
        "bow = tfidf.transform([s])\n",
        "print(f'BoW Shape: {bow.shape}')\n",
        "bow = bow.toarray()   # From sparse matrix to dense matrix (Careful with MEMORY)\n",
        "print(f'BoW Vector: {bow}')"
      ],
      "metadata": {
        "id": "HKJ5A7RDBffU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_bow_float(tfidf,bow[0])"
      ],
      "metadata": {
        "id": "GZs-oHI9BJwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display the IDF of some words.\n",
        "\n",
        "* High IDF = word that appears in few documents\n",
        "* Low IDF = word that appears in most of documents"
      ],
      "metadata": {
        "id": "3Q4sLbKUCLBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = tfidf.get_feature_names_out()\n",
        "word = input('Word: ').lower()\n",
        "\n",
        "if word in words:\n",
        "    k = list(words).index(word)\n",
        "    print(f'IDF({words[k]}) = {tfidf.idf_[k]}')\n",
        "else:\n",
        "    print('Not in vocabulary')"
      ],
      "metadata": {
        "id": "K5pF6HjdCLwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "More than one TF_IDF:\n",
        "\n",
        "There is a family of TF-IDF formulas.\n",
        "\n",
        "Another example is the **sublinear TF**, which is then:\n",
        "\n",
        "$\n",
        "\\begin{align}\n",
        "\\textrm{TF(w, d) = 1 + log} \\left( raw count \\right)\n",
        "\\end{align}\n",
        "$"
      ],
      "metadata": {
        "id": "IElNGl_BChNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_sublinear = TfidfVectorizer(sublinear_tf=True)\n",
        "tfidf_sublinear.fit(nltk_sentences)"
      ],
      "metadata": {
        "id": "CQsJ6bjECokG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = nltk_sentences[0]\n",
        "\n",
        "print(f'Text: \"{s}\"')\n",
        "bow_sl = tfidf_sublinear.transform([s])\n",
        "print(f'BoW Shape: {bow_sl.shape}')\n",
        "bow_sl = bow_sl.toarray()   # From sparse matrix to dense matrix (Careful with MEMORY)\n",
        "print(f'BoW Vector: {bow_sl}')"
      ],
      "metadata": {
        "id": "MeoHK32XCr08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_bow_float(tfidf_sublinear, bow_sl[0])"
      ],
      "metadata": {
        "id": "O-tTecw-C2tC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = 'yet'\n",
        "\n",
        "index = words.tolist().index(word)\n",
        "\n",
        "bow = tfidf.transform([s]).toarray()\n",
        "\n",
        "print(f'Word: \"{word}\"')\n",
        "print(f'TF-IDF with Natural TF   = {bow[0][index]:0.4f}')\n",
        "print(f'TF-IDF with Sublinear TF = {bow_sl[0][index]:0.4f}')"
      ],
      "metadata": {
        "id": "V4ec0CqEC6BE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = 'yet'\n",
        "s = nltk_sentences[0]\n",
        "s = s + ' '.join(100 * [word])\n",
        "\n",
        "bow = tfidf.transform([s]).toarray()\n",
        "bow_sl = tfidf_sublinear.transform([s]).toarray()\n",
        "\n",
        "index = words.tolist().index(word)\n",
        "print(f'Word: \"{word}\"')\n",
        "print(f'TF-IDF with Natural TF   = {bow[0][index]:0.4f}')\n",
        "print(f'TF-IDF with Sublinear TF = {bow_sl[0][index]:0.4f}')"
      ],
      "metadata": {
        "id": "4e5OAprLC-lR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# World Count & Freguency"
      ],
      "metadata": {
        "id": "MPC2cRnDCr-k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# tqdm\n",
        "\n",
        "*   from tqdm.notebook import tqdm\n",
        "*   它用于在循环中显示进度条，方便用户了解代码执行的进度，尤其是在处理大量数据时。\n",
        "*   基本用法：括号包裹一个可迭代对象： for i in tqdm(range(10)):\n",
        "\n"
      ],
      "metadata": {
        "id": "dmaXfSuEeqjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "r = requests.get(\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\")\n",
        "imdb_tgz = r.content\n",
        "import io\n",
        "import re\n",
        "import tarfile\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "good_file = re.compile(r\"^aclImdb/(test|train)/(pos|neg)/.*\\.txt$\")\n",
        "\n",
        "with tarfile.open(fileobj=io.BytesIO(r.content), mode=\"r:gz\") as tgz:\n",
        "    all_members = tgz.getmembers()\n",
        "    data_files = list(filter(lambda x: x.isfile() and good_file.match(x.name) is not None, all_members))\n",
        "    for f in tqdm(data_files):\n",
        "        tgz.extract(f)"
      ],
      "metadata": {
        "id": "R4lICFtDDuqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_files\n",
        "train_data, test_data = load_files(\"./aclImdb/train\", encoding=\"utf-8\"), load_files(\"./aclImdb/test\", encoding=\"utf-8\")\n",
        "\n",
        "label2txt = {label: txt for label, txt in enumerate(train_data.target_names)}\n",
        "txt2label = {txt: label for label, txt in label2txt.items()}\n",
        "type(train_data)\n",
        "\n",
        "X_train, y_train = train_data.data, train_data.target\n",
        "X_test, y_test = test_data.data, test_data.target"
      ],
      "metadata": {
        "id": "hmosWvnzCrsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Data"
      ],
      "metadata": {
        "id": "Bw2kE3LZEafW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "print(\"TRAIN data:\")\n",
        "print(\"class balance: \", np.bincount(y_train))\n",
        "print()\n",
        "print(\"TEST data:\")\n",
        "print(\"class balance: \", np.bincount(y_train))"
      ],
      "metadata": {
        "id": "nLYJHhW2EcDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# eli5 & gensim\n",
        "**后期 notebook will add more info about these 2 package**\n",
        "* eli5: 这主要用于帮助调试和解释机器学习模型。它可以揭示模型内部的工作原理，例如哪些特征对模型的预测贡献最大，这对于理解和信任模型非常有用。\n",
        "\n",
        "* gensim: 这是一个专注于主题建模和自然语言处理（NLP）的Python库。它能够处理大型文本语料库，并提供了一系列算法，例如Latent Semantic Analysis (LSA)、Latent Dirichlet Allocation (LDA) 和 Word2Vec 等，用于发现文本数据中的潜在结构、相似性以及词语之间的关系。"
      ],
      "metadata": {
        "id": "IlycjJPyLLMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install eli5\n",
        "!pip install gensim\n",
        "from gensim.corpora import Dictionary"
      ],
      "metadata": {
        "id": "zZSNbyNiE0f3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How many word in our vocabulary?**"
      ],
      "metadata": {
        "id": "94ShMs3fFSgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tokenized=[word_tokenize(x) for x in tqdm(X_train)]"
      ],
      "metadata": {
        "id": "Q1EyrSshFXDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d=Dictionary(X_train_tokenized)"
      ],
      "metadata": {
        "id": "mbC7FacJFh_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# rich\n",
        "做表example：\n",
        "\n",
        "     from rich.console import Console\n",
        "\n",
        "     from rich.table import Table\n",
        "\n",
        "     table.add_column(' ', juetify=' ',style=' ')\n",
        "\n",
        "justify : 对齐方式-----'left'(默认) 'right' 'center': 内容对齐位置"
      ],
      "metadata": {
        "id": "QWsQ_mFFHUT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import rich\n",
        "\n",
        "from rich.console import Console\n",
        "from rich.table import Table\n",
        "\n",
        "N = 20\n",
        "\n",
        "table = Table(title=f\"Top-{N} Most Frequent Tokens ({d.num_docs} documents, {d.num_pos} tokens, {len(d)} words in dictionary)\")\n",
        "\n",
        "table.add_column(\"Token\", justify=\"left\", style=\"black\")\n",
        "table.add_column(\"Corpus Frequency\", justify=\"right\")\n",
        "table.add_column(\"% of Tokens\", justify=\"right\")\n",
        "table.add_column(\"Document Frequency\", justify=\"right\")\n",
        "table.add_column(\"% of Documents\", justify=\"right\")\n",
        "\n",
        "for token, frequency in d.most_common(n=N):\n",
        "    percent_tokens = frequency / d.num_pos\n",
        "    doc_frequency = d.dfs[d.token2id[token]]\n",
        "    percent_doc = doc_frequency / d.num_docs\n",
        "    table.add_row(token, str(frequency), f\"{percent_tokens:.1%}\", str(doc_frequency), f\"{percent_doc:.1%}\")\n",
        "\n",
        "console = Console()\n",
        "console.print(table)"
      ],
      "metadata": {
        "id": "7zXvzPeOJMZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classify with Logistic Regression\n",
        "**X_train 转化为 bow 才可回归**"
      ],
      "metadata": {
        "id": "d-j-iE3sKKjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vec=CountVectorizer(lowercase=False,token_pattern=None,analyzer=word_tokenize)\n",
        "X_train_bow=vec.fit_transform(X_train)\n",
        "\n",
        "X_train_bow"
      ],
      "metadata": {
        "id": "L3BkJOEGKXXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "生成X_train_bow  稀疏矩阵第一行（即第一个训练文档）中，所有非零元素的列索引，列索引代表了在第一个文档中出现过的词汇表中的词语的 ID。"
      ],
      "metadata": {
        "id": "VJY-uyMWMRWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_bow[0].indices"
      ],
      "metadata": {
        "id": "RYT8Do_vL1i9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "console = Console(record=True, width=40)\n",
        "table = Table(title=f\"Tokens in sample\")\n",
        "\n",
        "table.add_column(\"Token ID\", justify=\"right\")\n",
        "table.add_column(\"Token\", justify=\"left\")\n",
        "table.add_column(\"Count\", justify=\"right\")\n",
        "\n",
        "words = vec.get_feature_names_out()\n",
        "for token_id, _ in zip(sorted(X_train_bow[0].indices), range(20)):\n",
        "    table.add_row(str(token_id), words[token_id], str(X_train_bow[0][0, token_id]))\n",
        "\n",
        "console.print(table)\n",
        "console.save_svg(\"bow.svg\", title=\"Bag of Words\")"
      ],
      "metadata": {
        "id": "8QjUhyu9MK3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**加入TF-IDF 进table above**\n",
        "\n",
        "用TfidfVectorizer"
      ],
      "metadata": {
        "id": "Woszjmg-Oto4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vec_tfidf = TfidfVectorizer(\n",
        "    lowercase=False,\n",
        "    token_pattern=None,\n",
        "    analyzer=word_tokenize\n",
        ")\n",
        "\n",
        "X_train_tfidf = vec_tfidf.fit_transform(X_train)\n",
        "X_train_tfidf"
      ],
      "metadata": {
        "id": "Fz08e76cPO8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "console = Console(record=True, width=80)\n",
        "table = Table(title=f\"Tokens in sample\")\n",
        "\n",
        "table.add_column(\"Token ID\", justify=\"right\")\n",
        "table.add_column(\"Token\", justify=\"left\")\n",
        "table.add_column(\"Count (TF)\", justify=\"right\")\n",
        "table.add_column(\"Doc Frequency (% corpus)\", justify=\"right\")\n",
        "table.add_column(\"TF-IDF\", justify=\"right\")\n",
        "\n",
        "words = vec.get_feature_names_out()\n",
        "for token_id, _ in zip(sorted(X_train_bow[0].indices), range(20)):\n",
        "    table.add_row(str(token_id), words[token_id], f\"{X_train_bow[0][0, token_id]}\", f\"{d.dfs[d.token2id[words[token_id]]] / d.num_docs:.3%}\", f\"{X_train_tfidf[0][0, token_id]:.3f}\")\n",
        "\n",
        "console.print(table)\n",
        "console.save_svg(\"tfidf.svg\", title=\"Bag of Words - TF-IDF\")"
      ],
      "metadata": {
        "id": "F_VbY_EOPaM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic Regression**"
      ],
      "metadata": {
        "id": "OSnWUJ6vPmxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "reg=LogisticRegression(max_iter=1000)\n",
        "reg.fit(X_train_bow,y_train)"
      ],
      "metadata": {
        "id": "msQDzZCqPsTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_bow=vec.transform(X_test)"
      ],
      "metadata": {
        "id": "t0bCe4WbP74m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_true=y_test,y_pred=reg.predict(X_test_bow)))"
      ],
      "metadata": {
        "id": "lqF20orgQMgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Kjc7c_Dm8RA"
      },
      "source": [
        "# Which words indicate a positive review ?\n",
        "使用-eli5-解释回归reg的权重"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhOXpP6HnAsf"
      },
      "source": [
        "* Let's use the coefficients of logistic regression\n",
        "* $x_i$ are token counts, we know $x_i>0$\n",
        "* $\\alpha_i > 0$ = the presence of the $i$-th word of dictionary indicates a positive review (because it increases the probability that $x$ belongs to the positive class)\n",
        "* $\\alpha_i < 0$ = the presence of the $i$-th word of dictionary indicates a negative review (because it increases the probability that $x$ belongs to the negative class)\n",
        "* $\\alpha_i>0, \\alpha_j > 0$ and $\\alpha_i > \\alpha_j$ = the $i$-th word of dictionary is **more** associated to a positive review than the $j$-th word\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import eli5\n",
        "vec.get_feature_names = vec.get_feature_names_out\n",
        "eli5.show_weights(reg,vec=vec,top=10,target_names=['negative','positive'])"
      ],
      "metadata": {
        "id": "6fKFyxRDQgAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eli5.explain_prediction(reg, X_test[0], vec=vec, target_names=['negative', 'positive'])"
      ],
      "metadata": {
        "id": "OnCiB7xcRRmP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}